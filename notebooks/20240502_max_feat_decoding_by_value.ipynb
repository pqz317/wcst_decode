{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask: Is decoding accuracy for max feature higher when max feature value is higher?\n",
    "Would expect that this is true if decoder is actually picking up on things\n",
    "Alg:\n",
    "- load decoding models (8 for 8 splits)\n",
    "- load Session data using seed from decoder (same train/test splits)\n",
    "- for each session_data subselect trials by value bin (use quantile values)\n",
    "- evaluate test accuracy on subset of trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status\n",
    "Not enough balanced sessions to make this work I don't think..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.behavioral_utils as behavioral_utils\n",
    "import utils.information_utils as information_utils\n",
    "import utils.visualization_utils as visualization_utils\n",
    "import utils.pseudo_classifier_utils as pseudo_classifier_utils\n",
    "import utils.io_utils as io_utils\n",
    "\n",
    "import utils.glm_utils as glm_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import utils.spike_utils as spike_utils\n",
    "import utils.subspace_utils as subspace_utils\n",
    "from trial_splitters.condition_trial_splitter import ConditionTrialSplitter \n",
    "from utils.session_data import SessionData\n",
    "\n",
    "from constants.behavioral_constants import *\n",
    "from constants.decoding_constants import *\n",
    "from itertools import cycle\n",
    "\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_INTERVAL = 500\n",
    "POST_INTERVAL = 500\n",
    "INTERVAL_SIZE = 50\n",
    "NUM_BINS_SMOOTH = 1\n",
    "EVENT = \"FixationOnCross\"\n",
    "SESS_SPIKES_PATH = \"/data/patrick_res/firing_rates/{sess_name}_firing_rates_{pre_interval}_{event}_{post_interval}_{interval_size}_bins_1_smooth.pickle\"\n",
    "\n",
    "DATA_MODE = \"FiringRate\"\n",
    "\n",
    "OUTPUT_DIR = \"/data/patrick_res/hyak/pseudo\"\n",
    "\n",
    "\n",
    "num_bins = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split(split, beh, bin_idx):\n",
    "    def filt_trials(row):\n",
    "        test_trials = row[\"TestTrials\"]\n",
    "        filt_test_trials = beh[\n",
    "            (beh.TrialNumber.isin(test_trials)) & \n",
    "            (beh.MaxValueBin == bin_idx)\n",
    "        ].TrialNumber.unique()\n",
    "        if len(filt_test_trials) < 1: \n",
    "            print(\"nooo\")\n",
    "        return pd.Series({\"Condition\": row.Condition, \"TrainTrials\": [], \"TestTrials\": filt_test_trials})\n",
    "    filt_split = split.apply(filt_trials, axis=1)\n",
    "    return filt_split\n",
    "\n",
    "\n",
    "def load_session_data(row, bin_idx):\n",
    "    # find splits that were used for the decoding\n",
    "    sess_name = row.session_name\n",
    "    behavior_path = SESS_BEHAVIOR_PATH.format(sess_name=sess_name)\n",
    "    beh = pd.read_csv(behavior_path)\n",
    "    valid_beh = behavioral_utils.get_valid_trials(beh)\n",
    "    feature_selections = behavioral_utils.get_selection_features(valid_beh)\n",
    "    valid_beh = pd.merge(valid_beh, feature_selections, on=\"TrialNumber\", how=\"inner\")\n",
    "    beh = behavioral_utils.get_feature_values_per_session(sess_name, valid_beh)\n",
    "    beh = behavioral_utils.get_max_feature_value(beh)\n",
    "    num_trials_per_feat = beh.groupby(\"MaxFeat\").TrialNumber.nunique()\n",
    "    num_feats = len(num_trials_per_feat)\n",
    "    if num_feats < 12 or np.any(num_trials_per_feat.values < 5):\n",
    "        return None\n",
    "\n",
    "    # load firing rates\n",
    "    spikes_path = SESS_SPIKES_PATH.format(\n",
    "        sess_name=sess_name, \n",
    "        pre_interval=PRE_INTERVAL, \n",
    "        event=EVENT, \n",
    "        post_interval=POST_INTERVAL, \n",
    "        interval_size=INTERVAL_SIZE\n",
    "    )\n",
    "    frs = pd.read_pickle(spikes_path)\n",
    "    frs = frs.groupby([\"UnitID\", \"TrialNumber\"]).mean().reset_index()\n",
    "    # hacky, but just pretend there's one timebin. \n",
    "    frs[\"TimeBins\"] = 0\n",
    "    frs = frs.rename(columns={DATA_MODE: \"Value\"})\n",
    "\n",
    "    splitter = ConditionTrialSplitter(beh, \"MaxFeat\", TEST_RATIO, seed=DECODER_SEED)\n",
    "    session_data = SessionData(sess_name, beh, frs, splitter)\n",
    "    session_data.pre_generate_splits(NUM_SPLITS)\n",
    "\n",
    "    beh = behavioral_utils.get_max_feature_value(beh, num_bins=num_bins, quantize_bins=True)\n",
    "    custom_splits  = [filter_split(next(session_data.splitter), beh, bin_idx) for _ in range(NUM_SPLITS)]\n",
    "    session_data.splitter = cycle(custom_splits)\n",
    "\n",
    "    return session_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bin 0\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "nooo\n",
      "21\n",
      "evaluating models for bin idx 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filt_session_data))\n\u001b[1;32m     12\u001b[0m time_bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m accs \u001b[38;5;241m=\u001b[39m \u001b[43mpseudo_classifier_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model_with_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilt_session_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m accs_by_value_bin[bin_idx, :] \u001b[38;5;241m=\u001b[39m accs\n",
      "File \u001b[0;32m/src/wcst_decode/utils/pseudo_classifier_utils.py:119\u001b[0m, in \u001b[0;36mevaluate_model_with_data\u001b[0;34m(models_by_bin, sess_datas, time_bins, num_train_per_cond, num_test_per_cond)\u001b[0m\n\u001b[1;32m    116\u001b[0m split_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_idx, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# assumes models, splits are ordered the same\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     pseudo_sess \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43msess_datas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_pseudo_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_train_per_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test_per_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    123\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m pseudo_sess[pseudo_sess\u001b[38;5;241m.\u001b[39mType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m     x_test \u001b[38;5;241m=\u001b[39m transform_input_data(test_data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/src/wcst_decode/utils/pseudo_classifier_utils.py:120\u001b[0m, in \u001b[0;36mevaluate_model_with_data.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    116\u001b[0m split_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_idx, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# assumes models, splits are ordered the same\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     pseudo_sess \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(sess_datas\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_pseudo_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_train_per_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test_per_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalues, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    123\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m pseudo_sess[pseudo_sess\u001b[38;5;241m.\u001b[39mType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m     x_test \u001b[38;5;241m=\u001b[39m transform_input_data(test_data)\n",
      "File \u001b[0;32m/src/wcst_decode/utils/session_data.py:70\u001b[0m, in \u001b[0;36mSessionData.generate_pseudo_data\u001b[0;34m(self, num_train, num_test, time_bin)\u001b[0m\n\u001b[1;32m     68\u001b[0m frs_at_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrs[np\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrs\u001b[38;5;241m.\u001b[39mTimeBins, time_bin)]\n\u001b[1;32m     69\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter)\n\u001b[0;32m---> 70\u001b[0m pseudo_pop \u001b[38;5;241m=\u001b[39m \u001b[43mpseudo_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_pseudo_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrs_at_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m pseudo_pop[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess_name\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# NOTE: very hacky way of giving unique ID to units across sessions\u001b[39;00m\n",
      "File \u001b[0;32m/src/wcst_decode/utils/pseudo_utils.py:44\u001b[0m, in \u001b[0;36mgenerate_pseudo_population\u001b[0;34m(frs, split, num_train_samples, num_test_samples, rng)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(train_trials)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mchoice(train_trials, num_train_samples \u001b[38;5;241m*\u001b[39m num_units)\n\u001b[0;32m---> 44\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m samples_for_conditions\u001b[38;5;241m.\u001b[39mappend(train_samples)\n\u001b[1;32m     46\u001b[0m samples_for_conditions\u001b[38;5;241m.\u001b[39mappend(test_samples)\n",
      "File \u001b[0;32m_generator.pyx:729\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "valid_sess = pd.read_pickle(SESSIONS_PATH)\n",
    "models = np.load(\n",
    "    os.path.join(OUTPUT_DIR, f\"intertrial_agg_max_feat_models.npy\"), \n",
    "    allow_pickle=True\n",
    ")\n",
    "accs_by_value_bin = np.empty((num_bins, NUM_SPLITS))\n",
    "for bin_idx in range(num_bins):\n",
    "    print(f\"Processing bin {bin_idx}\")\n",
    "    filt_session_data = valid_sess.apply(lambda x: load_session_data(x, bin_idx), axis=1)\n",
    "    filt_session_data = filt_session_data.dropna()\n",
    "    print(len(filt_session_data))\n",
    "    time_bins = np.zeros(1)\n",
    "    accs = pseudo_classifier_utils.evaluate_model_with_data(models, filt_session_data, time_bins)\n",
    "    accs_by_value_bin[bin_idx, :] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
